{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Catalog","text":"<p>Catalog is a Python command-line-interface (CLI) that automates discovery and understanding of Forest Service geospatial and tabular data. It harvests XML metadata and MapServer service definitions from three anonymized portals\u2014Research Data Archive (RDA), Geospatial Data Discovery (GDD), and FSGeodata Clearinghouse (FSGeodata)\u2014builds embeddings, and allows the user to explore datasets with a semantic, RAG-powered search that can answer questions like \u201cWhat data exists and how do I use it?\u201d.</p>"},{"location":"#why-it-matters","title":"Why it matters","text":"<ul> <li>Hunting across portals, downloading XML one-by-one, and reconciling service URLs slows research and product teams.</li> <li>Explaining lineage and \u201cfit for purpose\u201d to stakeholders is hard without a unified view.</li> <li>Traditional keyword search misses nuance; semantic search with LLMs locates relevant datasets faster.</li> </ul>"},{"location":"#what-catalog-does","title":"What Catalog does","text":"<ul> <li>Automated harvesting from RDA, GDD, and FSGeodata (XML + MapServer JSON).</li> <li>Embeds metadata with a vector database (table) and uses LLMs in a Retrieval-Augmented Generation (RAG) flow for semantic Q&amp;A.</li> <li>Outputs organized metadata and service URLs you can plug into dashboards or analyses.</li> </ul>"},{"location":"#the-process-at-a-glance","title":"The process (at a glance)","text":"<pre><code>flowchart TB\n  Sources[RDA / GDD / FSGeodata] --&gt; Download\n  Download --&gt; Normalize[\"Normalize metadata (XML + JSON)\"]\n  Normalize --&gt; Embed[VectorDB embeddings]\n  Embed --&gt; RAG[LLM + RAG pipeline]\n  RAG --&gt; Answers[Semantic search &amp; dataset guidance]\n</code></pre> <ol> <li> <p>Identify metadata sources:  </p> <ul> <li>Research Archive (RDA): research-grade datasets from the agency's research directory.  </li> <li>Geospatial Discovery (GDD): current operational GIS layers and services.  </li> <li>FSGeodata Clearinghouse (FSGeodata): authoritative basemaps, boundaries, operational layers, and raster products.</li> </ul> </li> <li> <p>Harvest metadata: <code>uv run catalog download-fs-metadata</code> pulls XML and MapServer JSON, normalizes fields, and stores metadata for indexing.</p> </li> <li> <p>Build the vector database (chromadb): embeddings go into vector storage; metadata stays linked.</p> </li> <li> <p>RAG-based search: the CLI uses the embeddings plus an LLM to answer dataset and lineage questions with grounded citations.</p> </li> </ol>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Vision and philosophy: see <code>docs/vision.md</code>.</li> <li>Disclaimer and project status: see <code>docs/disclaimer.md</code>.</li> <li>Architecture and design decisions: see <code>docs/architecture.md</code>.</li> <li>Data sources deep dive: see <code>docs/data-sources.md</code>.</li> <li>CLI usage and examples: see <code>docs/cli.md</code>.</li> <li>Vector DB details and comparisons: see <code>docs/vector-db.md</code>.</li> </ul>"},{"location":"architecture/","title":"Application Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>Catalog is a Python CLI application that aggregates geospatial metadata from multiple USFS data repositories into a unified, searchable catalog. The system implements a Retrieval-Augmented Generation (RAG) pipeline combining vector-based semantic search with LLM-powered natural language discovery.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              CLI Layer                                  \u2502\n\u2502                             (cli.py)                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                           \u2502                           \u2502\n        \u25bc                           \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data Loaders \u2502         \u2502  Vector Store   \u2502         \u2502   LLM Client    \u2502\n\u2502   (usfs.py)   \u2502         \u2502   (core.py)     \u2502         \u2502   (bots.py)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                          \u2502                           \u2502\n        \u25bc                          \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Schema     \u2502         \u2502    ChromaDB     \u2502         \u2502     Ollama      \u2502\n\u2502  (schema.py)  \u2502         \u2502                 \u2502         \u2502      API        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#component-descriptions","title":"Component Descriptions","text":""},{"location":"architecture/#cli-layer-clipy","title":"CLI Layer (<code>cli.py</code>)","text":"<p>The entry point for all user interactions. Built with Click framework, it exposes six commands:</p> Command Purpose <code>health</code> System health check <code>download_fs_metadata</code> Fetch raw metadata from all sources <code>build_fs_catalog</code> Parse metadata into unified JSON catalog <code>build_fs_chromadb</code> Index catalog into vector database <code>query_fs_chromadb</code> Semantic search queries <code>ollama_chat</code> LLM-augmented natural language discovery"},{"location":"architecture/#data-loaders-usfspy","title":"Data Loaders (<code>usfs.py</code>)","text":"<p>Orchestrates metadata collection from three federal data sources:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         USFS Class                              \u2502\n\u2502              (Orchestrator for all data sources)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                       \u2502                           \u2502\n        \u25bc                       \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 FSGeodataLoader\u2502      \u2502GeospatialData \u2502       \u2502   RDALoader   \u2502\n\u2502               \u2502       \u2502  Discovery    \u2502       \u2502               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Source: EDW   \u2502       \u2502 Source: Hub   \u2502       \u2502 Source: RDS   \u2502\n\u2502 Format: XML   \u2502       \u2502 Format: DCAT  \u2502       \u2502 Format: JSON  \u2502\n\u2502 Protocol: HTTP\u2502       \u2502 Protocol: HTTP\u2502       \u2502 Protocol: HTTP\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>FSGeodataLoader</p> <ul> <li>Scrapes dataset index from <code>data.fs.usda.gov</code></li> <li>Downloads FGDC-compliant XML metadata files</li> <li>Retrieves associated MapServer service descriptors</li> <li>Implements rate limiting (250ms delay)</li> </ul> <p>GeospatialDataDiscovery</p> <ul> <li>Fetches DCAT-US 1.1 JSON feed from ArcGIS Hub</li> <li>Single-endpoint bulk download</li> <li>Parses standardized federal open data format</li> </ul> <p>RDALoader</p> <ul> <li>Queries Research Data Archive JSON API</li> <li>Downloads research dataset metadata</li> <li>Handles scientific provenance information</li> </ul>"},{"location":"architecture/#schema-layer-schemapy","title":"Schema Layer (<code>schema.py</code>)","text":"<p>Defines the unified data model using Pydantic:</p> <pre><code>USFSDocument\n\u251c\u2500\u2500 id: str           # SHA-256 hash of normalized title\n\u251c\u2500\u2500 title: str        # Dataset title\n\u251c\u2500\u2500 abstract: str     # Summary description\n\u251c\u2500\u2500 purpose: str      # Intended use (FSGeodata only)\n\u251c\u2500\u2500 description: str  # General description\n\u251c\u2500\u2500 keywords: list    # Subject keywords\n\u251c\u2500\u2500 src: str          # Source identifier\n\u2514\u2500\u2500 lineage: list     # Processing history\n</code></pre> <p>Provides:</p> <ul> <li>Data validation on ingest</li> <li>Serialization to/from JSON</li> <li>Markdown rendering for output display</li> </ul>"},{"location":"architecture/#vector-store-corepy","title":"Vector Store (<code>core.py</code>)","text":"<p>Manages semantic search capabilities via ChromaDB:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      ChromaVectorDB                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  load_document_metadata()  \u2502 Load catalog JSON                  \u2502\n\u2502  batch_load_documents()    \u2502 Index documents in batches of 100  \u2502\n\u2502  query()                   \u2502 Semantic similarity search         \u2502\n\u2502  extract_lineage_info()    \u2502 Format lineage for embedding       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Embedding Strategy</p> <p>Documents are embedded as concatenated text:</p> <pre><code>Title: {title}\nAbstract: {abstract}\nPurpose: {purpose}\nSource: {src}\nKeywords: {keywords}\nLineage: {lineage}\n</code></pre> <p>Query Flow</p> <ol> <li>User query is embedded using same model</li> <li>Cosine distance computed against all documents</li> <li>Top-k results returned with distance scores</li> <li>Lower distance = higher semantic relevance</li> </ol>"},{"location":"architecture/#llm-client-botspy","title":"LLM Client (<code>bots.py</code>)","text":"<p>Integrates with Ollama API for natural language responses:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        OllamaBot                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  System Prompt: \"Data librarian\" persona                        \u2502\n\u2502  Input: Question + Vector search context                        \u2502\n\u2502  Output: Synthesized natural language response                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>RAG Pipeline</p> <ol> <li>User submits natural language question</li> <li>Question used for vector search (retrieve top-k documents)</li> <li>Retrieved documents formatted as context</li> <li>LLM generates response grounded in catalog data</li> </ol>"},{"location":"architecture/#utilities-libpy","title":"Utilities (<code>lib.py</code>)","text":"<p>Shared helper functions:</p> Function Purpose <code>save_json()</code> Serialize data to JSON file <code>clean_str()</code> Normalize whitespace and Unicode <code>hash_string()</code> Generate SHA-256 document IDs"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#ingestion-pipeline","title":"Ingestion Pipeline","text":"<pre><code>1. Download Phase\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502FSGeodata \u2502    \u2502   GDD    \u2502    \u2502   RDA    \u2502\n   \u2502   XML    \u2502    \u2502   JSON   \u2502    \u2502   JSON   \u2502\n   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502               \u2502               \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u25bc\n2. Parse Phase     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502 Loaders  \u2502\n                   \u2502 parse()  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n3. Harmonize      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 USFSDocument \u2502\n                  \u2502    Schema    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n4. Store          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 catalog.json \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n5. Index          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502   ChromaDB   \u2502\n                  \u2502   Vectors    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#query-pipeline","title":"Query Pipeline","text":"<pre><code>User Query\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Vector Search   \u2502 \u25c4\u2500\u2500\u2500 ChromaDB\n\u2502 (top-k results) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                         \u2502\n         \u25bc                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Direct Results  \u2502      \u2502  RAG Pipeline   \u2502\n\u2502 (query command) \u2502      \u2502 (chat command)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502   Ollama LLM    \u2502\n                         \u2502   (synthesis)   \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502 Natural Language\u2502\n                         \u2502    Response     \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>catalog/\n\u251c\u2500\u2500 src/catalog/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 cli.py          # Click CLI commands\n\u2502   \u251c\u2500\u2500 usfs.py         # Data loaders (USFS, FSGeodata, GDD, RDA)\n\u2502   \u251c\u2500\u2500 core.py         # ChromaDB vector store\n\u2502   \u251c\u2500\u2500 schema.py       # Pydantic data models\n\u2502   \u251c\u2500\u2500 bots.py         # Ollama LLM client\n\u2502   \u2514\u2500\u2500 lib.py          # Utility functions\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 usfs/\n\u2502       \u251c\u2500\u2500 fsgeodata/  # FSGeodata XML + service JSON\n\u2502       \u2502   \u251c\u2500\u2500 metadata/\n\u2502       \u2502   \u2514\u2500\u2500 services/\n\u2502       \u251c\u2500\u2500 gdd/        # GDD DCAT JSON\n\u2502       \u251c\u2500\u2500 rda/        # RDA JSON\n\u2502       \u2514\u2500\u2500 catalog.json # Unified catalog\n\u251c\u2500\u2500 chromadb/           # Vector database storage\n\u2514\u2500\u2500 docs/               # Documentation\n</code></pre>"},{"location":"architecture/#dependencies","title":"Dependencies","text":"Package Purpose click CLI framework chromadb Vector database ollama LLM API client pydantic Data validation beautifulsoup4 XML/HTML parsing requests HTTP client rich Console formatting python-dotenv Environment configuration"},{"location":"architecture/#configuration","title":"Configuration","text":"<p>Environment variables (<code>.env</code>):</p> <pre><code>OLLAMA_API_KEY=&lt;api-key&gt;\nOLLAMA_API_URL=&lt;ollama-server-url&gt;\nOLLAMA_MODEL=&lt;model-name&gt;\n</code></pre>"},{"location":"article-outline/","title":"Journal Article Outline","text":""},{"location":"article-outline/#title-suggested","title":"Title (suggested)","text":"<p>\"A Retrieval-Augmented Generation Approach for Discovering Heterogeneous Federal Geospatial Metadata\"</p>"},{"location":"article-outline/#1-introduction","title":"1. Introduction","text":"<ul> <li>Problem: Federal geospatial data fragmented across repositories with incompatible schemas</li> <li>Motivation: Researchers struggle to discover relevant USFS datasets using keyword search</li> <li>Contribution: A RAG-based system that harmonizes metadata and enables semantic discovery</li> <li>Research questions:</li> <li>Can schema harmonization improve cross-repository search?</li> <li>Does vector-based semantic search outperform keyword matching for geospatial metadata?</li> </ul>"},{"location":"article-outline/#2-background-related-work","title":"2. Background &amp; Related Work","text":"<ul> <li>Geospatial metadata standards (ISO 19115, DCAT-US, FGDC)</li> <li>Federal data discovery challenges (data.gov limitations)</li> <li>RAG architectures for information retrieval</li> <li>Vector databases for document search</li> </ul>"},{"location":"article-outline/#3-data-sources","title":"3. Data Sources","text":"<ul> <li>FSGeodata Clearinghouse (EDW) - XML/FGDC format</li> <li>Geospatial Data Discovery (GDD) - DCAT-US 1.1 JSON via ArcGIS Hub</li> <li>Research Data Archive (RDA) - Custom JSON API</li> <li>Characterization of each source's schema, coverage, and limitations</li> </ul>"},{"location":"article-outline/#4-methods","title":"4. Methods","text":"<ul> <li>Schema harmonization approach</li> <li>Vector embedding and indexing pipeline</li> <li>RAG architecture with LLM integration</li> <li>System implementation</li> </ul> <p>See <code>methods-section.md</code> for the detailed draft.</p>"},{"location":"article-outline/#5-results","title":"5. Results","text":"<ul> <li>Catalog statistics (document counts, field coverage)</li> <li>Query evaluation (semantic vs keyword search examples)</li> <li>User study or expert evaluation (if applicable)</li> </ul>"},{"location":"article-outline/#suggested-analyses","title":"Suggested Analyses","text":"<ul> <li>Table: Document counts by source</li> <li>Table: Field completeness rates across sources</li> <li>Figure: Query response comparison (keyword vs semantic)</li> <li>Example queries demonstrating semantic understanding</li> </ul>"},{"location":"article-outline/#6-discussion","title":"6. Discussion","text":"<ul> <li>Implications for federal data discovery</li> <li>Limitations:</li> <li>Embedding model choices and their impact on retrieval quality</li> <li>Information loss during schema harmonization</li> <li>Dependency on source API stability</li> <li>Generalizability to other federal repositories (EPA, NOAA, USGS)</li> </ul>"},{"location":"article-outline/#7-conclusion-future-work","title":"7. Conclusion &amp; Future Work","text":"<ul> <li>Summary of contributions</li> <li>Extensions:</li> <li>Additional data sources</li> <li>Fine-tuned embedding models for geospatial terminology</li> <li>Quantitative evaluation metrics (precision, recall, NDCG)</li> <li>User interface development</li> </ul>"},{"location":"article-outline/#target-journals-suggestions","title":"Target Journals (suggestions)","text":"<ul> <li>Computers &amp; Geosciences</li> <li>International Journal of Geographical Information Science</li> <li>Environmental Modelling &amp; Software</li> <li>Journal of the Association for Information Science and Technology (JASIST)</li> <li>Earth Science Informatics</li> </ul>"},{"location":"demos/","title":"Demo Command Line Usage","text":""},{"location":"demos/#semantic-query-with-no-ai-interaction","title":"Semantic Query with No AI Interaction","text":"Terminal"},{"location":"demos/#a-query-using-vector-db-results-augmented-using-ai-llm","title":"A Query Using Vector DB, Results Augmented using AI LLM","text":"Terminal"},{"location":"disclaimer/","title":"Disclaimer","text":""},{"location":"disclaimer/#project-status","title":"Project Status","text":"<p>Catalog is an independent proof-of-concept research project. It is not an official USDA Forest Service product, system, or endorsed tool. It does not represent the position, policy, or direction of the USDA, the Forest Service, or any other federal agency.</p>"},{"location":"disclaimer/#data-sources","title":"Data Sources","text":"<p>All metadata harvested by this tool is drawn exclusively from publicly accessible federal open data sources:</p> <ul> <li>FSGeodata Clearinghouse \u2014 publicly available geospatial metadata</li> <li>Geospatial Data Discovery Hub \u2014 DCAT-US 1.1 open data feed</li> <li>Research Data Archive \u2014 publicly published research dataset metadata</li> </ul> <p>No internal, sensitive, restricted, or non-public USFS data is accessed, stored, or processed by this tool. All source data is already available to the public via the above portals.</p>"},{"location":"disclaimer/#ai-and-compute-resources","title":"AI and Compute Resources","text":"<p>This project was initiated during a period when USDA/USFS policy restricted employee use of AI tools. The approach was designed specifically to work within that constraint:</p> <ul> <li>AI/LLM inference runs locally (via Ollama) or on CyVerse, an NSF-funded public research cyberinfrastructure platform operated by the University of Arizona and available to the broader research community.</li> <li>No federal data is transmitted to commercial AI APIs.</li> <li>All project code resides on a personal workstation or a CyVerse virtual machine\u2014not on USFS-managed systems.</li> </ul> <p>USDA/USFS AI use policies have since evolved. This project's approach\u2014public data only, on-premises or academic compute, no commercial API data transfer\u2014remains consistent with prudent data handling regardless of current policy.</p>"},{"location":"disclaimer/#intellectual-property-and-liability","title":"Intellectual Property and Liability","text":"<ul> <li>This software is provided as-is, without warranty of any kind.</li> <li>The project author(s) make no claims regarding the completeness, accuracy, or fitness for purpose of the harvested metadata. Metadata accuracy is the responsibility of the originating data portals.</li> <li>Use of this tool for operational or decision-making purposes should be validated against authoritative sources.</li> </ul>"},{"location":"disclaimer/#open-source-and-transparency","title":"Open Source and Transparency","text":"<p>All source code is publicly available. The methods, data sources, and AI integrations are documented so that the approach can be independently reviewed, reproduced, or adapted.</p>"},{"location":"methods/","title":"Methods","text":""},{"location":"methods/#data-sources-and-collection","title":"Data Sources and Collection","text":"<p>We developed automated harvesters for three USFS geospatial data repositories, each employing distinct metadata standards and access mechanisms.</p> <p>FSGeodata Clearinghouse. The Enterprise Data Warehouse (EDW) datasets are accessed via the USFS Geodata portal (https://data.fs.usda.gov/geodata/edw/datasets.php). We implemented a web scraper using BeautifulSoup to parse the datasets index page, extracting links to XML metadata files conforming to FGDC Content Standard for Digital Geospatial Metadata. For each dataset, we retrieve both the metadata XML and, where available, associated ArcGIS MapServer service descriptors in JSON format. A rate limiter (250ms delay) ensures compliance with server policies.</p> <p>Geospatial Data Discovery (GDD). The USFS ArcGIS Hub portal exposes a DCAT-US 1.1 compliant feed at a single JSON endpoint (https://data-usfs.hub.arcgis.com/api/feed/dcat-us/1.1.json). This feed provides dataset titles, descriptions, keywords, and thematic classifications in a standardized federal open data format.</p> <p>Research Data Archive (RDA). The USFS Research Data Archive provides a JSON web service (https://www.fs.usda.gov/rds/archive/webservice/datagov) returning dataset metadata including titles, descriptions, and keyword arrays. This source emphasizes research datasets with scientific provenance.</p>"},{"location":"methods/#schema-harmonization","title":"Schema Harmonization","text":"<p>To enable cross-repository search, we defined a unified document schema (<code>USFSDocument</code>) with the following fields:</p> Field Type Description <code>id</code> string SHA-256 hash of normalized title (lowercase, trimmed) <code>title</code> string Dataset title <code>abstract</code> string Summary description (mapped from FGDC abstract or DCAT description) <code>purpose</code> string Intended use statement (FSGeodata only) <code>description</code> string General description text <code>keywords</code> array Subject keywords (from themekey, keyword, or theme fields) <code>src</code> string Source identifier: \"fsgeodata\", \"gdd\", or \"rda\" <code>lineage</code> array Processing history with dates (FSGeodata only) <p>The <code>id</code> field serves as a deduplication key, ensuring datasets appearing in multiple repositories are not double-counted. Text fields undergo normalization including whitespace collapsing and Unicode standardization via a <code>clean_str()</code> utility function.</p> <p>Schema Mapping. Each source requires distinct parsing logic:</p> <ul> <li>FSGeodata: XML parsing extracts <code>&lt;title&gt;</code>, <code>&lt;descript&gt;&lt;abstract&gt;</code>, <code>&lt;descript&gt;&lt;purpose&gt;</code>, <code>&lt;themekey&gt;</code>, and <code>&lt;procstep&gt;</code> elements</li> <li>GDD: JSON mapping from DCAT fields (<code>title</code>, <code>description</code>, <code>keyword</code>, <code>theme</code>)</li> <li>RDA: Direct JSON field extraction (<code>title</code>, <code>description</code>, <code>keyword</code>)</li> </ul>"},{"location":"methods/#vector-embedding-and-storage","title":"Vector Embedding and Storage","text":"<p>Harmonized documents are loaded into ChromaDB, an open-source vector database. For each document, we construct an embedding input string concatenating:</p> <pre><code>Title: {title}\nAbstract: {abstract}\nPurpose: {purpose}\nSource: {src}\nKeywords: {keywords}\nLineage: {lineage}\n</code></pre> <p>ChromaDB's default embedding model generates vector representations stored alongside document metadata. Documents are processed in batches of 100 to optimize memory usage. The collection is rebuilt from scratch on each indexing operation to ensure consistency.</p>"},{"location":"methods/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<p>The system supports two query modes:</p> <p>Vector Search. Users submit natural language queries, which are embedded and compared against the document collection using cosine distance. The top-k results (configurable, default k=5) are returned with relevance distances, where lower distances indicate higher semantic similarity.</p> <p>LLM-Augmented Discovery. For complex discovery questions, we implement a RAG pipeline:</p> <ol> <li>The user query is used to retrieve the top-k relevant documents via vector search</li> <li>Retrieved documents are formatted as context</li> <li>The query and context are submitted to an LLM (configurable via Ollama API)</li> <li>The LLM generates a natural language response synthesizing the search results</li> </ol> <p>The LLM system prompt frames the model as a \"data librarian\" with instructions to:</p> <ul> <li>List relevant datasets with explanations of relevance</li> <li>Prioritize results by distance score (lower = more relevant)</li> <li>Provide direct yes/no answers for existence queries</li> <li>Avoid speculation beyond catalog contents</li> </ul>"},{"location":"methods/#implementation","title":"Implementation","text":"<p>The system is implemented in Python as a CLI tool using the Click framework. Key dependencies include:</p> <ul> <li>ChromaDB for vector storage and similarity search</li> <li>Ollama client for LLM integration</li> <li>BeautifulSoup for XML/HTML parsing</li> <li>Pydantic for schema validation</li> <li>Requests for HTTP operations</li> </ul> <p>The modular architecture separates concerns: data loaders (<code>usfs.py</code>), schema definitions (<code>schema.py</code>), vector operations (<code>core.py</code>), and LLM integration (<code>bots.py</code>).</p>"},{"location":"methods/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Data Collection                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   FSGeodata     \u2502        GDD          \u2502          RDA            \u2502\n\u2502   (XML/FGDC)    \u2502    (DCAT-US 1.1)    \u2502        (JSON)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                   \u2502                       \u2502\n         \u25bc                   \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Schema Harmonization                         \u2502\n\u2502                      (USFSDocument)                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Unified Catalog (JSON)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Vector Embedding &amp; Indexing                     \u2502\n\u2502                        (ChromaDB)                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Vector Search       \u2502     \u2502      LLM-Augmented Discovery    \u2502\n\u2502   (Semantic Queries)    \u2502     \u2502     (Natural Language Q&amp;A)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"vision/","title":"Vision: A Pragmatic Search Engine for USFS Data","text":""},{"location":"vision/#the-core-idea","title":"The Core Idea","text":"<p>Catalog is not an enterprise software project. It is a focused, practical tool built on a simple premise: the data and metadata you need already exist\u2014what's missing is a unified way to find and understand them.</p> <p>The Forest Service maintains authoritative geospatial data across several portals\u2014FSGeodata Clearinghouse, the Geospatial Data Discovery hub, and the Research Data Archive. Each portal serves a purpose, but none of them talk to each other. Users who need to find the right dataset must know which portal to look in, understand each portal's search interface, and reconcile what they find across incompatible metadata schemas.</p> <p>Catalog addresses this by treating those existing portals as the source of truth and building a lightweight discovery layer on top of them.</p>"},{"location":"vision/#what-this-is-and-isnt","title":"What This Is (and Isn't)","text":"This tool is... This tool is not... A search engine over existing USFS metadata A replacement for existing USFS data portals A thin integration layer An enterprise data management system Locally runnable by individuals or small teams A centrally hosted service Built on open, existing standards (FGDC, DCAT-US) A new metadata standard Extensible as AI capabilities evolve A finished product requiring sustained IT investment"},{"location":"vision/#the-dont-rebuild-it-philosophy","title":"The \"Don't Rebuild It\" Philosophy","text":"<p>Large federal IT projects frequently attempt to consolidate data by building new systems\u2014new databases, new APIs, new user interfaces. These projects are expensive, slow, and often fail to keep pace with the underlying data and technology trends.</p> <p>Catalog takes the opposite approach:</p> <ol> <li>Harvest, don't replace. Metadata is downloaded directly from live sources. There is no separate data store to maintain or keep synchronized beyond running the harvest commands.</li> <li>Harmonize at the edge. Schema differences between FSGeodata XML, GDD DCAT-US JSON, and RDA JSON are resolved at ingest time into a minimal common model. The original source data is preserved.</li> <li>Use existing AI infrastructure. The tool integrates with Ollama, which can run entirely on-premises. No cloud accounts, no data leaving the network.</li> </ol>"},{"location":"vision/#catalog-as-an-internal-usfs-search-engine","title":"Catalog as an Internal USFS Search Engine","text":"<p>From a user's perspective, Catalog behaves like a search engine scoped to USFS data holdings. It is working towards answering questions that today require navigating multiple portals manually:</p> <ul> <li>\"What elevation datasets are available for the Pacific Northwest?\"</li> <li>\"Is there a road centerline layer that covers the Tongass National Forest?\"</li> <li>\"What data exists about fuels treatment history and how do I access it as a service?\"</li> </ul> <p>These queries go beyond simple keyword matching. Catalog uses vector embeddings and a Retrieval-Augmented Generation (RAG) pipeline so that semantically related datasets surface even when the exact words don't match. An LLM then synthesizes a grounded response with citations to the actual metadata records.</p> <p>This positions Catalog as an internal knowledge tool\u2014something a GIS analyst, researcher, or data steward can run locally to understand what data exists, where it lives, and how to use it.</p>"},{"location":"vision/#the-ai-dimension","title":"The AI Dimension","text":"<p>Catalog's architecture is designed to grow with AI capabilities rather than bet on a specific technology:</p> <ul> <li>Embeddings are pluggable. ChromaDB supports swapping embedding models. As domain-specific geospatial embedding models emerge, they can be substituted without changing the pipeline.</li> <li>LLM integration is local-first. Using Ollama keeps the tool viable in restricted-network environments and avoids sending sensitive internal data to external APIs.</li> <li>The RAG pattern scales. As the metadata corpus grows\u2014additional portals, richer metadata fields, tabular data inventories\u2014the same retrieval and generation architecture handles it.</li> </ul> <p>Longer-term, this foundation supports more advanced use cases: automated data lineage summaries, fit-for-purpose recommendations, discovery across agency boundaries (USGS, NOAA, EPA), and integration into analyst workflows as a conversational assistant.</p>"},{"location":"vision/#current-implementation-proof-of-concept","title":"Current Implementation: Proof of Concept","text":"<p>The current implementation is command-line driven. Every capability\u2014harvesting metadata, building the vector database, querying it, and chatting with the LLM\u2014is exposed as a CLI command that a user runs locally on their machine. This is intentional for a proof of concept: it keeps the system simple, removes infrastructure dependencies, and lets the core ideas be validated quickly without committing to a deployment model.</p> <p>The CLI design does not constrain the architecture. The vector database querying and LLM integration are already isolated in discrete modules (<code>core.py</code> and <code>bots.py</code>). Moving those capabilities behind a REST API or web service would be a straightforward next step\u2014the logic stays the same and the CLI becomes one of several possible clients rather than the only one.</p> <p>A server-based implementation would unlock use cases the CLI cannot support:</p> <ul> <li>A web interface where analysts query the catalog without installing anything</li> <li>Shared, centrally maintained vector index updated on a schedule</li> <li>Integration with existing USFS applications and portals via API</li> <li>Multi-user access with authentication and usage tracking</li> </ul> <p>The proof-of-concept phase exists to validate that the underlying approach\u2014harvesting existing metadata, embedding it, and querying it with an LLM\u2014produces useful results before investing in that infrastructure. If it does, the path to a hosted service is clear.</p>"},{"location":"vision/#why-this-approach-makes-sense-for-usfs","title":"Why This Approach Makes Sense for USFS","text":"<ul> <li>Low barrier to adoption. The tool runs on a laptop with a single <code>uv run</code> command. There is no infrastructure to provision.</li> <li>No lock-in. The output is standard JSON and a local ChromaDB directory. The tool does not require any proprietary platform.</li> <li>Respects existing investments. The portals that data stewards already maintain\u2014FSGeodata, GDD, RDA\u2014remain the authoritative sources. Catalog amplifies them rather than competing with them.</li> <li>Demonstrates AI value concretely. Rather than abstract AI strategy, Catalog shows a specific, working example of how LLMs improve geospatial data discovery for Forest Service users today.</li> </ul>"},{"location":"vision/#origins-and-policy-context","title":"Origins and Policy Context","text":"<p>This project was started at a time when USDA/USFS policy restricted employee use of AI tools. The design reflects that constraint deliberately: all metadata sources are public federal open data, AI inference runs locally or on CyVerse (an NSF-funded academic compute platform), and no data is sent to commercial AI APIs. That approach remains sound independent of current policy\u2014it keeps sensitive data handling simple and the tool auditable.</p> <p>USDA/USFS AI policy has since evolved.</p> <p>For full details on data sources, compute resources, and project status, see the disclaimer.</p>"},{"location":"search_methods/bot/","title":"BOT Searches","text":""},{"location":"search_methods/hybrid_search/","title":"Hybrid Search and rank_bm25","text":""},{"location":"search_methods/hybrid_search/#hybrid-search","title":"Hybrid Search","text":"<p>Hybrid search combines two retrieval strategies into one system:</p> <ul> <li>Keyword/sparse search (e.g., BM25) -- great at exact matches like error codes, SKUs, proper names, and domain-specific terms.</li> <li>Semantic/dense vector search (e.g., embeddings via Sentence Transformers) -- great at understanding meaning, synonyms, and conceptual similarity.</li> </ul> <p>Neither alone covers all query types. Hybrid search runs both in parallel, then fuses the results using methods like:</p> <ul> <li>Reciprocal Rank Fusion (RRF) -- merges based on rank positions, avoiding score-scale mismatch: <code>RRF_score = sum(1 / (k + rank_i))</code></li> <li>Weighted linear combination -- normalizes scores to [0,1] and blends: <code>final = alpha * semantic + (1 - alpha) * keyword</code></li> </ul>"},{"location":"search_methods/hybrid_search/#rank_bm25","title":"rank_bm25","text":"<p>A pure-Python library implementing the BM25 ranking algorithm (the same algorithm powering Elasticsearch/Solr). It provides three variants:</p> Class Best for <code>BM25Okapi</code> Standard choice, most widely used <code>BM25L</code> When long documents are penalized too much <code>BM25Plus</code> Ensures every term match contributes positively (good for short chunks in RAG) <p>Key characteristics:</p> <ul> <li>No external search engine needed -- runs entirely in memory.</li> <li>You must handle tokenization yourself (lowercasing, stopwords, stemming).</li> <li>Simple API: <code>bm25.get_scores(query)</code> and <code>bm25.get_top_n(query, docs, n=k)</code>.</li> </ul>"},{"location":"search_methods/hybrid_search/#how-they-fit-together","title":"How They Fit Together","text":"<p><code>rank_bm25</code> serves as the keyword component in a hybrid search pipeline:</p> <pre><code>Query --&gt; [rank_bm25 BM25Okapi]    --&gt; keyword ranked list  \\\n      --&gt; [Vector DB + embeddings] --&gt; semantic ranked list  }--&gt; Fusion --&gt; Final results\n</code></pre> <p>LangChain's <code>EnsembleRetriever</code> wraps this pattern with <code>BM25Retriever</code> (uses rank_bm25 internally) combined with any vector retriever, with configurable weights.</p> <p>Note: <code>rank_bm25</code> is low-maintenance. A faster alternative called bm25s exists using scipy sparse matrices, but rank_bm25 remains the most commonly used due to its simplicity and LangChain integration.</p>"},{"location":"search_methods/simple/","title":"Simple Searches","text":""}]}